{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import time, os, pathlib, random, re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import Generator, Discriminator, initialize_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento de GAN, utilizando Wasserstein Loss + Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./logs/wgan-gp-mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_text('texto_inicial', \n",
    "'Este é um treinamento de Wgan utilizando o dataset MNIST e aplicando as equações de Wasserstein Loss + Gradient Penalty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baixando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mnist = MNIST(root='./', train=True, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# VARIÁVEIS\n",
    "CHANNEL_NOISE = 1\n",
    "NOISE_DIM = 100\n",
    "IMG_CHANNEL = 1\n",
    "FEATURES = 16\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "MODELS_DIR = './models'\n",
    "TAXA_TREINAMENTO_DISCRIMINATOR = 5  # ou seja, o discriminator treina 5 vezes mais que o generator\n",
    "LAMBDA_GP = 10 # TAXA DO GRADIENT PENALTY\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.min()=tensor(-1.), dataset.max()=tensor(1.)\n",
      "Novo shape: dataset.shape=torch.Size([60000, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Refatorando o dataset e incluindo mais uma dimensão (como se fosse um canal) para as redes neurais.\n",
    "dataset = dataset_mnist.data\n",
    "N, H, W = dataset.size()\n",
    "dataset = dataset.view((N, 1, H, W))\n",
    "\n",
    "dataset = dataset / 255. # normalizando os dados entre 0 e 1\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5)) # depois normalizando entre -1 e 1\n",
    "])\n",
    "dataset = transformer(dataset)\n",
    "\n",
    "print (f'{dataset.min()=}, {dataset.max()=}')\n",
    "print (f'Novo shape: {dataset.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiznado no tensorboard algumas imagens\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "imgs_tensor = next(iter(dataloader))\n",
    "grid = make_grid(imgs_tensor, nrow=16, padding=0, normalize=True)\n",
    "writer.add_image('real_imgs', grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funões úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(b_size, device_ = torch.device('cpu')):\n",
    "    return torch.randn((b_size, NOISE_DIM, 1, 1), device=device_)\n",
    "\n",
    "def list_models():\n",
    "    list_ = list(pathlib.Path(MODELS_DIR).glob('*.pt'))\n",
    "    if (len(list_) > 0):\n",
    "        aux = []\n",
    "        for model_path in list_:\n",
    "            model_path = str(model_path)\n",
    "            epoch = int(re.findall(r'[0-9]{1,}', model_path)[0])\n",
    "            aux.append([model_path, epoch])\n",
    "        \n",
    "        # Colocando a lista em ordem decrescente de épocas (da maior época em primeiro para a menor)\n",
    "        aux = sorted([[mp, ep] for mp, ep in aux], key=lambda item: item[1], reverse=True)\n",
    "        return aux\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def carregar_treinamento():\n",
    "    lista = list_models()\n",
    "    \n",
    "    print ('Criando os modelos...')\n",
    "    generator_ = Generator(channel_noise=NOISE_DIM, channel_img=IMG_CHANNEL, features=FEATURES)\n",
    "    discriminator_ = Discriminator(channels_img=IMG_CHANNEL, img_size=64, features=FEATURES)\n",
    "\n",
    "    if (lista):\n",
    "        last_checkpoint_path, last_epoch_ = lista[0]\n",
    "        last_checkpoint = torch.load(last_checkpoint_path, map_location=torch.device('cpu'))\n",
    "        \n",
    "        print (f'Carregando o último treinamento. {last_epoch_=}')\n",
    "        print ('generator: ', generator_.load_state_dict(last_checkpoint['generator_state_dict']))\n",
    "        print ('discriminator: ', discriminator_.load_state_dict(last_checkpoint['discriminator_state_dict']))\n",
    "        fixed_noise_ = last_checkpoint['fixed_noise']\n",
    "\n",
    "    else:\n",
    "        print ('Iniciando os pesos dos modelos.')\n",
    "        generator_.apply(initialize_weights)\n",
    "        discriminator_.apply(initialize_weights)\n",
    "        fixed_noise_ = get_noise(64)\n",
    "        last_epoch_ = 0\n",
    "    \n",
    "    return generator_, discriminator_, fixed_noise_, last_epoch_\n",
    "\n",
    "def salvar_modelos(model_g, model_d, fixed_noise, epoch_):\n",
    "    checkpoint = {\n",
    "        'generator_state_dict': model_g.state_dict(),\n",
    "        'discriminator_state_dict': model_d.state_dict(),\n",
    "        'fixed_noise': fixed_noise\n",
    "    }\n",
    "    torch.save(checkpoint, MODELS_DIR + f'/checkpoin_{str(epoch_).zfill(4)}.pt')\n",
    "\n",
    "def manter_somente_n_ultimos_modelos(n_ultimos):\n",
    "    lista = list_models()\n",
    "    for checkpoint_path, _ in lista[n_ultimos:]:\n",
    "        try:\n",
    "            os.remove(checkpoint_path)\n",
    "        except:\n",
    "            print (checkpoint_path + ' já não existia...')\n",
    "\n",
    "def gradient_penalty(model_d, real_imgs, fake_imgs, device_):\n",
    "\n",
    "    b_size, c, h, w = real_imgs.shape\n",
    "    alpha = torch.rand((b_size, 1, 1, 1)).repeat(1, c, h, w).to(device_)\n",
    "\n",
    "    interpolated_imgs = real_imgs * alpha + fake_imgs * (1-alpha)\n",
    "\n",
    "    # Cálculo score\n",
    "    mixed_score = model_d(interpolated_imgs)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_imgs,\n",
    "        outputs = mixed_score,\n",
    "        grad_outputs = torch.ones_like(mixed_score),\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    penalty = torch.mean((gradient_norm-1) ** 2)\n",
    "    \n",
    "    return penalty\n",
    "\n",
    "def checkpoint_image(model_g, fixed_noise_, epoch_):\n",
    "    with torch.no_grad():\n",
    "            fake = model_g(fixed_noise_)\n",
    "            fake_grid = make_grid(fake, nrow=8, padding=0, normalize=True)\n",
    "            writer.add_image(f'fake_img_{epoch}', fake_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_generator.shape=torch.Size([4, 1, 64, 64])\n",
      "output_discriminator.shape=torch.Size([4, 1, 1, 1])\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(channel_noise=NOISE_DIM, channel_img=IMG_CHANNEL, features=FEATURES)\n",
    "noise = get_noise(4)\n",
    "output_generator = generator(noise)\n",
    "print (f'{output_generator.shape=}')\n",
    "# Repara que aqui, a imagem de saída da rede generator é 64x64, que deve ser a entrada da rede discriminator\n",
    "\n",
    "discriminator = Discriminator(channels_img=IMG_CHANNEL, img_size=64, features=FEATURES)\n",
    "output_discriminator = discriminator(output_generator)\n",
    "print (f'{output_discriminator.shape=}')\n",
    "\n",
    "generator.apply(initialize_weights)\n",
    "discriminator.apply(initialize_weights)\n",
    "\n",
    "print ('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataloader)=469\n",
      "Criando os modelos...\n",
      "Iniciando os pesos dos modelos.\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print (f'{len(dataloader)=}')\n",
    "generator, discriminator, fixed_noise, last_epoch = carregar_treinamento()\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "fixed_noise = fixed_noise.to(device)\n",
    "\n",
    "optim_generator = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0., 0.9))\n",
    "optim_discriminator = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0., 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [02:09<00:00,  3.63it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n",
      "100%|██████████| 469/469 [02:09<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "discriminator.train()\n",
    "generator.train()\n",
    "\n",
    "for epoch in range(last_epoch, last_epoch + 15 + 1, 1):\n",
    "    \n",
    "    for real_imgs in tqdm(dataloader):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        b_size = len(real_imgs)\n",
    "\n",
    "        # Treinando o discriminator\n",
    "        for _ in range(TAXA_TREINAMENTO_DISCRIMINATOR):\n",
    "            noise = get_noise(b_size, device)\n",
    "            fake_imgs = generator(noise)\n",
    "\n",
    "            output_real = discriminator(real_imgs)\n",
    "            output_fake = discriminator(fake_imgs)\n",
    "\n",
    "            # Cálculo do Gradient-penalty\n",
    "            gp = gradient_penalty(discriminator, real_imgs, fake_imgs, device)\n",
    "            loss_discriminator = -(torch.mean(output_real.view(-1)) - torch.mean(output_fake.view(-1))) + LAMBDA_GP*gp\n",
    "            discriminator.zero_grad()\n",
    "            loss_discriminator.backward(retain_graph=True)\n",
    "            optim_discriminator.step()\n",
    "        \n",
    "        # Treinando o generator\n",
    "        output_fake_for_generator = discriminator(fake_imgs)\n",
    "        loss_generator = -torch.mean(output_fake_for_generator.view(-1))\n",
    "        generator.zero_grad()\n",
    "        loss_generator.backward()\n",
    "        optim_generator.step()\n",
    "\n",
    "    # Levando as variáveis para o tensorboard\n",
    "    writer.add_scalar('loss_discriminator', loss_discriminator.item(), epoch)\n",
    "    writer.add_scalar('loss_generator', loss_generator.item(), epoch)\n",
    "\n",
    "    salvar_modelos(generator, discriminator, fixed_noise, epoch)\n",
    "    manter_somente_n_ultimos_modelos(3)\n",
    "    checkpoint_image(generator, fixed_noise, epoch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa39838d5afd7d94b7544cb5e5351cace91d1e0eb74b6451fdb6f11f3a068bed"
  },
  "kernelspec": {
   "display_name": "principal:Python",
   "language": "python",
   "name": "conda-env-principal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
